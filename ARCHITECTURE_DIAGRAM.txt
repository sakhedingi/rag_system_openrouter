================================================================================
                    RAG SYSTEM ARCHITECTURE DIAGRAM
================================================================================

                              USER INTERFACE
                    ┌─────────────────────────────┐
                    │   Streamlit Web App         │
                    │   - Chat Interface          │
                    │   - File Upload             │
                    │   - Mode Selection          │
                    └──────────────┬──────────────┘
                                   │
                                   │ User Query
                                   ▼
================================================================================
                          OPTIMIZED RAG ENGINE
================================================================================
                    ┌─────────────────────────────┐
                    │   OptimizedRAG Controller   │
                    │   (optimized_rag.py)        │
                    └──────────────┬──────────────┘
                                   │
                    ┌──────────────┴──────────────┐
                    │   Query Processing Flow     │
                    └──────────────┬──────────────┘
                                   │
        ┌──────────────────────────┼──────────────────────────┐
        │                          │                          │
        ▼                          ▼                          ▼
┌───────────────┐          ┌──────────────┐         ┌─────────────────┐
│  STEP 1:      │          │  STEP 2:     │         │  STEP 3:        │
│  Check Cache  │          │  Check       │         │  Vector Search  │
│               │          │  Memory      │         │                 │
│  ┌─────────┐ │          │  ┌────────┐  │         │  ┌───────────┐  │
│  │ Prompt  │ │          │  │Context │  │         │  │ Semantic  │  │
│  │ Cache   │ │          │  │Memory  │  │         │  │ Search    │  │
│  │ (CAG)   │ │          │  │Store   │  │         │  │ Engine    │  │
│  └─────────┘ │          │  └────────┘  │         │  └───────────┘  │
│               │          │              │         │                 │
│  Exact Match? │          │  Similar     │         │  Top-K Chunks   │
│  └─► Yes: Return        │  Questions?  │         │  by Similarity  │
│      Cached Answer      │  └─► Reuse   │         │                 │
│                         │      Context  │         │                 │
└───────────────┘          └──────────────┘         └─────────────────┘
        │                          │                          │
        │ No Match                 │ Retrieved                │ Retrieved
        │                          │ Contexts                 │ Chunks
        └──────────────────────────┴──────────────────────────┘
                                   │
                                   ▼
                    ┌──────────────────────────────┐
                    │  STEP 4: Combine Context     │
                    │  - Past Answers (Memory)     │
                    │  - Document Chunks (Vector)  │
                    │  - System Prompt             │
                    └──────────────┬───────────────┘
                                   │
                                   ▼
                    ┌──────────────────────────────┐
                    │  STEP 5: AI Generation       │
                    │  ┌────────────────────────┐  │
                    │  │  OpenRouter API        │  │
                    │  │  (Claude 3.5 Sonnet)   │  │
                    │  │  - Streaming Response  │  │
                    │  └────────────────────────┘  │
                    └──────────────┬───────────────┘
                                   │
                                   ▼
                    ┌──────────────────────────────┐
                    │  STEP 6: Cache & Store       │
                    │  - Save to Prompt Cache      │
                    │  - Save to Context Memory    │
                    │  - Update Statistics         │
                    └──────────────┬───────────────┘
                                   │
                                   ▼
                            Response to User

================================================================================
                        STORAGE LAYER COMPONENTS
================================================================================

┌─────────────────────┐  ┌─────────────────────┐  ┌─────────────────────┐
│  VECTOR STORE       │  │  PROMPT CACHE       │  │  CONTEXT MEMORY     │
│  MANAGER            │  │  (CAG)              │  │  STORE              │
├─────────────────────┤  ├─────────────────────┤  ├─────────────────────┤
│                     │  │                     │  │                     │
│ • Pre-vectorized    │  │ • Cached Q&A pairs  │  │ • Past contexts     │
│   document chunks   │  │ • Query hashes      │  │ • Conversation      │
│                     │  │ • Response text     │  │   threads           │
│ • Embedding vectors │  │ • Token savings     │  │ • Confidence scores │
│   (1536 dimensions) │  │ • Access counts     │  │ • Tags & metadata   │
│                     │  │ • Timestamps        │  │ • Relations         │
│ • Metadata:         │  │                     │  │                     │
│   - Source file     │  │ Storage:            │  │ Storage:            │
│   - Chunk index     │  │ SQLite Database     │  │ SQLite Database     │
│   - File hash       │  │ .cag_cache/         │  │ .memory_store/      │
│   - Timestamp       │  │ prompts.db          │  │ contexts.db         │
│                     │  │                     │  │                     │
│ Storage:            │  │ Tables:             │  │ Tables:             │
│ Pickle + JSON       │  │ • cached_prompts    │  │ • context_memory    │
│ .vector_cache/      │  │ • context_chunks    │  │ • conversation_     │
│ vectors.pkl         │  │                     │  │   threads           │
│ metadata.json       │  │                     │  │ • context_relations │
│                     │  │                     │  │                     │
└──────────┬──────────┘  └──────────┬──────────┘  └──────────┬──────────┘
           │                        │                         │
           └────────────────────────┴─────────────────────────┘
                                    │
                                    ▼
================================================================================
                          KNOWLEDGE BASE LAYER
================================================================================

                    ┌──────────────────────────────┐
                    │   Document Processing        │
                    │   (semantic_search.py)       │
                    ├──────────────────────────────┤
                    │                              │
                    │  1. Load Documents           │
                    │     • PDF (pdfminer)         │
                    │     • DOCX (python-docx)     │
                    │     • TXT (plain text)       │
                    │                              │
                    │  2. Chunk Text               │
                    │     • 1000 chars per chunk   │
                    │     • 200 char overlap       │
                    │                              │
                    │  3. Generate Embeddings      │
                    │     • OpenRouter API         │
                    │     • text-embedding-3-small │
                    │                              │
                    │  4. Store Vectors            │
                    │     • Pickle serialization   │
                    │     • Metadata tracking      │
                    │                              │
                    └──────────────┬───────────────┘
                                   │
                                   ▼
                    ┌──────────────────────────────┐
                    │   ./knowledge_base/          │
                    │   ├── Document1.pdf          │
                    │   ├── Document2.docx         │
                    │   ├── Document3.txt          │
                    │   └── ...                    │
                    └──────────────────────────────┘

================================================================================
                          EXTERNAL SERVICES
================================================================================

                    ┌──────────────────────────────┐
                    │   OpenRouter API             │
                    │   (openrouter.ai)            │
                    ├──────────────────────────────┤
                    │                              │
                    │  Chat Models:                │
                    │  • Claude 3.5 Sonnet         │
                    │  • GPT-4                     │
                    │  • Gemini Pro                │
                    │  • And more...               │
                    │                              │
                    │  Embedding Models:           │
                    │  • text-embedding-3-small    │
                    │  • text-embedding-3-large    │
                    │                              │
                    └──────────────────────────────┘

================================================================================
                          DATA FLOW SUMMARY
================================================================================

User Query
    │
    ├─► [1] Check Prompt Cache ──► Cache Hit? ──► Return Cached Answer
    │                                   │
    │                                   No
    │                                   │
    ├─► [2] Check Context Memory ──► Similar Questions? ──► Retrieve Context
    │                                   │
    │                                   │
    ├─► [3] Vector Search ──────────► Find Top-K Chunks ──► Retrieve Chunks
    │                                   │
    │                                   │
    └─► [4] Combine All Context ────► [5] Generate Answer ──► [6] Cache Result
                                          │
                                          │
                                          └─► Stream to User

================================================================================
                          OPTIMIZATION METRICS
================================================================================

┌─────────────────────────────────────────────────────────────────────────┐
│  Performance Improvements:                                              │
│                                                                         │
│  • Pre-Vectorization:     10-100x faster retrieval                     │
│  • Prompt Caching:        60-80% reduction in API costs                │
│  • Context Memory:        Improved answer quality over time            │
│  • Document Chunking:     Better precision in retrieval                │
│                                                                         │
│  Token Savings Example:                                                │
│  • Without Cache:  1000 tokens/query × 100 queries = 100,000 tokens   │
│  • With Cache:     1000 tokens × 20 new queries = 20,000 tokens       │
│  • Savings:        80% reduction (80,000 tokens saved)                 │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘

================================================================================
